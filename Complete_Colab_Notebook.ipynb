{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìö Multi-modal PDF RAG with LangGraph - Complete Google Colab Guide\n",
        "\n",
        "**Run this on GPU for best performance!**\n",
        "\n",
        "This notebook provides a complete multi-modal RAG system that can:\n",
        "- Process PDFs with text, images, and tables\n",
        "- Perform semantic search on both text and images  \n",
        "- Answer questions using a multi-agent system\n",
        "- Use Hugging Face models (free, no OpenAI required)\n",
        "\n",
        "## ‚öôÔ∏è Setup Steps:\n",
        "1. **Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí GPU (T4 or better)\n",
        "2. **Get Hugging Face API key**: https://huggingface.co/settings/tokens\n",
        "3. **Run all cells in order**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install All Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install all required packages\n",
        "!pip install -q langchain>=0.1.0 langchain-openai>=0.0.2 langchain-community>=0.0.10 langgraph>=0.0.20\n",
        "!pip install -q unstructured[pdf] pypdf pdf2image Pillow\n",
        "!pip install -q chromadb faiss-cpu\n",
        "!pip install -q sentence-transformers torch torchvision\n",
        "!pip install -q duckduckgo-search tavily-python\n",
        "!pip install -q python-dotenv requests opencv-python\n",
        "!pip install -q numpy==1.24.3 pydantic>=2.7.4,<3.0.0\n",
        "\n",
        "# Install system dependencies\n",
        "!apt-get update -qq\n",
        "!apt-get install -y -qq poppler-utils tesseract-ocr\n",
        "\n",
        "print(\"‚úÖ All dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Set Your API Keys\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# ‚ö†Ô∏è REPLACE WITH YOUR ACTUAL API KEYS ‚ö†Ô∏è\n",
        "HUGGINGFACE_API_KEY = \"YOUR_HUGGINGFACE_API_KEY_HERE\"  # Required - Get from https://huggingface.co/settings/tokens\n",
        "TAVILY_API_KEY = \"YOUR_TAVILY_API_KEY_HERE\"  # Optional - For web search\n",
        "\n",
        "# Set environment variables\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = HUGGINGFACE_API_KEY\n",
        "os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY\n",
        "os.environ[\"USE_HUGGINGFACE_PRIMARY\"] = \"true\"\n",
        "os.environ[\"USE_OPENAI_EMBEDDINGS\"] = \"false\"\n",
        "os.environ[\"USE_OPENAI_FALLBACK\"] = \"false\"\n",
        "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
        "\n",
        "print(\"‚úÖ API keys configured!\")\n",
        "print(f\"‚úì Hugging Face API key set: {bool(HUGGINGFACE_API_KEY and HUGGINGFACE_API_KEY != 'YOUR_HUGGINGFACE_API_KEY_HERE')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create Configuration File\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create config.py\n",
        "config_code = '''\"\"\"Configuration settings for the Multi-modal RAG system.\"\"\"\n",
        "import os\n",
        "\n",
        "# API Keys\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
        "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\", \"\")\n",
        "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\", \"\")\n",
        "\n",
        "# Model Configuration\n",
        "LLM_MODEL = \"gpt-4-1106-preview\"\n",
        "VISION_MODEL = \"gpt-4-vision-preview\"\n",
        "EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
        "\n",
        "# Hugging Face Configuration\n",
        "HUGGINGFACE_LLM_MODEL = os.getenv(\"HUGGINGFACE_LLM_MODEL\", \"distilgpt2\")  # Use distilgpt2 for faster local loading\n",
        "HUGGINGFACE_MULTIMODAL_MODEL = \"Salesforce/blip-image-captioning-large\"\n",
        "USE_HUGGINGFACE_PRIMARY = os.getenv(\"USE_HUGGINGFACE_PRIMARY\", \"true\").lower() == \"true\"\n",
        "USE_OPENAI_EMBEDDINGS = os.getenv(\"USE_OPENAI_EMBEDDINGS\", \"false\").lower() == \"true\"\n",
        "USE_OPENAI_FALLBACK = os.getenv(\"USE_OPENAI_FALLBACK\", \"false\").lower() == \"true\"\n",
        "\n",
        "# Vector Store Configuration\n",
        "VECTOR_STORE_PATH = os.getenv(\"VECTOR_STORE_PATH\", \"./vector_store\")\n",
        "CHROMA_COLLECTION_NAME = \"multimodal_pdf_rag\"\n",
        "MAX_RETRIEVAL_DOCS = 3\n",
        "MAX_IMAGES_PER_QUERY = 2\n",
        "\n",
        "# PDF Processing Configuration\n",
        "PDF_PROCESSING_MODE = \"hi_res\"\n",
        "CHUNK_SIZE = 1000\n",
        "CHUNK_OVERLAP = 200\n",
        "\n",
        "# Agent Configuration\n",
        "MAX_ITERATIONS = 30\n",
        "TEMPERATURE = 0.0\n",
        "\n",
        "# Rate Limit Configuration\n",
        "MAX_RETRIES = 3\n",
        "RETRY_DELAY_SECONDS = 2\n",
        "'''\n",
        "\n",
        "with open('config.py', 'w') as f:\n",
        "    f.write(config_code)\n",
        "\n",
        "print(\"‚úÖ Created config.py\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Create Image Embeddings Module\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create image_embeddings.py\n",
        "image_embeddings_code = '''\"\"\"Image embedding module using CLIP for semantic image search.\"\"\"\n",
        "import base64\n",
        "from io import BytesIO\n",
        "from typing import List, Optional\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    CLIP_AVAILABLE = True\n",
        "except ImportError:\n",
        "    CLIP_AVAILABLE = False\n",
        "\n",
        "class ImageEmbedder:\n",
        "    \"\"\"Image embedding using CLIP model.\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name: str = \"clip-ViT-B-32\"):\n",
        "        self.model = None\n",
        "        self.model_name = model_name\n",
        "        if CLIP_AVAILABLE:\n",
        "            try:\n",
        "                import torch\n",
        "                device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "                self.model = SentenceTransformer(self.model_name, device=device)\n",
        "                print(f\"‚úì Loaded CLIP model: {self.model_name} on {device}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not load CLIP model: {e}\")\n",
        "    \n",
        "    def is_available(self) -> bool:\n",
        "        \"\"\"Check if CLIP is available.\"\"\"\n",
        "        return self.model is not None\n",
        "    \n",
        "    def embed_image(self, image_base64: str) -> Optional[np.ndarray]:\n",
        "        \"\"\"Generate embedding for a base64-encoded image.\"\"\"\n",
        "        if not self.is_available():\n",
        "            return None\n",
        "        \n",
        "        try:\n",
        "            # Decode base64 image\n",
        "            image_data = base64.b64decode(image_base64)\n",
        "            image = Image.open(BytesIO(image_data))\n",
        "            \n",
        "            # Generate embedding\n",
        "            embedding = self.model.encode(image, convert_to_numpy=True)\n",
        "            return embedding\n",
        "        except Exception as e:\n",
        "            print(f\"Error embedding image: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def embed_images(self, images_base64: List[str]) -> List[Optional[np.ndarray]]:\n",
        "        \"\"\"Generate embeddings for multiple images.\"\"\"\n",
        "        if not self.is_available():\n",
        "            return [None] * len(images_base64)\n",
        "        \n",
        "        embeddings = []\n",
        "        for img_b64 in images_base64:\n",
        "            emb = self.embed_image(img_b64)\n",
        "            embeddings.append(emb)\n",
        "        return embeddings\n",
        "'''\n",
        "\n",
        "with open('image_embeddings.py', 'w') as f:\n",
        "    f.write(image_embeddings_code)\n",
        "\n",
        "print(\"‚úÖ Created image_embeddings.py\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Clone Repository from GitHub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository from GitHub\n",
        "!git clone https://github.com/DevXSoni021/Multi_model_RAG_Langgraph.git\n",
        "%cd Multi_model_RAG_Langgraph\n",
        "\n",
        "print(\"‚úÖ Repository cloned successfully!\")\n",
        "print(\"üìÅ All Python files are now available in the current directory\")\n",
        "\n",
        "# FIX 1: Fix import errors in vector_store.py and agents.py\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Fix vector_store.py imports and numpy array issues\n",
        "with open('vector_store.py', 'r') as f:\n",
        "    vector_store_code = f.read()\n",
        "\n",
        "# Fix the Document import\n",
        "vector_store_code = vector_store_code.replace(\n",
        "    'from langchain.schema import Document',\n",
        "    'from langchain_core.documents import Document'\n",
        ")\n",
        "\n",
        "# Also fix any other langchain.schema imports\n",
        "vector_store_code = vector_store_code.replace(\n",
        "    'from langchain.schema',\n",
        "    'from langchain_core'\n",
        ")\n",
        "\n",
        "# Fix numpy array truth value check (ambiguous evaluation) - multiple patterns\n",
        "# Pattern 1: Direct check\n",
        "vector_store_code = vector_store_code.replace(\n",
        "    'if image_embedding:',\n",
        "    'if image_embedding is not None:'\n",
        ")\n",
        "# Pattern 2: With whitespace variations\n",
        "vector_store_code = vector_store_code.replace(\n",
        "    'if image_embedding :',\n",
        "    'if image_embedding is not None:'\n",
        ")\n",
        "# Pattern 3: In try-except blocks (if not already fixed)\n",
        "import re\n",
        "vector_store_code = re.sub(\n",
        "    r'if\\s+image_embedding\\s*:',\n",
        "    'if image_embedding is not None:',\n",
        "    vector_store_code\n",
        ")\n",
        "\n",
        "with open('vector_store.py', 'w') as f:\n",
        "    f.write(vector_store_code)\n",
        "\n",
        "print(\"‚úÖ Fixed vector_store.py imports and numpy array checks\")\n",
        "\n",
        "# Fix agents.py imports\n",
        "with open('agents.py', 'r') as f:\n",
        "    agents_imports_code = f.read()\n",
        "\n",
        "# Fix langchain.prompts imports\n",
        "agents_imports_code = agents_imports_code.replace(\n",
        "    'from langchain.prompts import',\n",
        "    'from langchain_core.prompts import'\n",
        ")\n",
        "\n",
        "# Fix langchain.tools imports\n",
        "agents_imports_code = agents_imports_code.replace(\n",
        "    'from langchain.tools import',\n",
        "    'from langchain_core.tools import'\n",
        ")\n",
        "\n",
        "# Fix langchain.agents imports (keep these as they might be needed)\n",
        "# But also add fallback for langchain_core\n",
        "\n",
        "with open('agents.py', 'w') as f:\n",
        "    f.write(agents_imports_code)\n",
        "\n",
        "print(\"‚úÖ Fixed agents.py imports\")\n",
        "\n",
        "# FIX 2: Fix agents.py - Apply critical fixes for state handling and infinite loops\n",
        "# Read agents.py\n",
        "with open('agents.py', 'r') as f:\n",
        "    agents_code = f.read()\n",
        "\n",
        "# FIX 1: Replace query method to handle state correctly\n",
        "new_query_method = '''    def query(self, question: str):\n",
        "        \"\"\"Query the RAG system - FIXED VERSION.\"\"\"\n",
        "        initial_state = {\n",
        "            \"question\": question,\n",
        "            \"documents\": \"\",\n",
        "            \"images\": [],\n",
        "            \"chat_history\": [],\n",
        "            \"image_query_triggered\": False,\n",
        "            \"answer\": \"\"  # Initialize answer field\n",
        "        }\n",
        "        \n",
        "        try:\n",
        "            # Use invoke() to get final merged state (not stream())\n",
        "            final_state = self.graph.invoke(initial_state)\n",
        "            \n",
        "            # Extract answer - handle different state structures\n",
        "            if isinstance(final_state, dict):\n",
        "                # Method 1: Direct answer field (from invoke)\n",
        "                answer = final_state.get(\"answer\", \"\")\n",
        "                if answer and answer.strip() and len(answer.strip()) > 10:\n",
        "                    return answer.strip()\n",
        "                \n",
        "                # Method 2: Nested under node name (from stream or node output)\n",
        "                if \"answer_generator\" in final_state:\n",
        "                    nested = final_state[\"answer_generator\"]\n",
        "                    if isinstance(nested, dict):\n",
        "                        nested_answer = nested.get(\"answer\", \"\")\n",
        "                        if nested_answer and nested_answer.strip() and len(nested_answer.strip()) > 10:\n",
        "                            return nested_answer.strip()\n",
        "                \n",
        "                # Method 3: From messages\n",
        "                if \"messages\" in final_state:\n",
        "                    messages = final_state[\"messages\"]\n",
        "                    for msg in reversed(messages):\n",
        "                        if hasattr(msg, \"content\") and msg.content:\n",
        "                            content = str(msg.content).strip()\n",
        "                            if len(content) > 10:\n",
        "                                return content\n",
        "                        elif isinstance(msg, str) and len(msg.strip()) > 10:\n",
        "                            return msg.strip()\n",
        "            \n",
        "            return \"No answer generated. Please try rephrasing your question.\"\n",
        "            \n",
        "        except Exception as e:\n",
        "            error_msg = str(e)\n",
        "            print(f\"Error in query: {error_msg[:200]}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return f\"Error: {error_msg[:200]}. Please check your configuration and try again.\"\n",
        "'''\n",
        "\n",
        "# FIX 2: Replace generate_answer to prevent infinite loops\n",
        "new_generate_answer = '''        def generate_answer(state):\n",
        "            \"\"\"Generate the final answer - FIXED VERSION.\"\"\"\n",
        "            question = state.get(\"question\", \"\")\n",
        "            documents = state.get(\"documents\", \"\")\n",
        "            images = state.get(\"images\", [])\n",
        "            chat_history = state.get(\"chat_history\", [])\n",
        "            \n",
        "            # CRITICAL: Prevent infinite loops\n",
        "            # Check if we're repeating the same question\n",
        "            if chat_history:\n",
        "                last_entries = chat_history[-3:] if len(chat_history) >= 3 else chat_history\n",
        "                question_count = sum(1 for entry in last_entries if entry[0] == \"user\" and entry[1] == question)\n",
        "                if question_count >= 2:\n",
        "                    return {\n",
        "                        \"answer\": \"I notice this question was already asked. Please try rephrasing or ask a different question.\",\n",
        "                        \"chat_history\": chat_history\n",
        "                    }\n",
        "            \n",
        "            # Limit chat history to prevent token overflow\n",
        "            if len(chat_history) > 6:\n",
        "                chat_history = chat_history[-6:]  # Keep only last 6 entries\n",
        "            \n",
        "            # If using OpenAI and images are available, use vision model\n",
        "            if self.primary_llm_type == \"openai\" and images:\n",
        "                try:\n",
        "                    image_messages = [{\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img}\"}\n",
        "                    } for img in images[:2]]  # Limit to 2 images\n",
        "                    \n",
        "                    vision_content = [\n",
        "                        {\"type\": \"text\", \"text\": f\"Context: {documents}\\\\nQuestion: {question}\"},\n",
        "                        *image_messages\n",
        "                    ]\n",
        "                    response = self.vision_llm.invoke([(\"user\", vision_content)])\n",
        "                    answer = response.content if hasattr(response, \"content\") else str(response)\n",
        "                except Exception as e:\n",
        "                    print(f\"Vision model error: {e}, falling back to text\")\n",
        "                    answer_chain = self.answer_agent\n",
        "                    response = answer_chain.invoke({\"question\": question, \"documents\": documents, \"images\": []})\n",
        "                    answer = response.content if hasattr(response, \"content\") else str(response)\n",
        "            else:\n",
        "                # Use standard LLM\n",
        "                answer_chain = self.answer_agent\n",
        "                try:\n",
        "                    response = answer_chain.invoke({\"question\": question, \"documents\": documents, \"images\": images})\n",
        "                    \n",
        "                    # Handle different response types\n",
        "                    if isinstance(response, str):\n",
        "                        answer = response\n",
        "                    elif hasattr(response, \"content\"):\n",
        "                        answer = response.content\n",
        "                    else:\n",
        "                        answer = str(response)\n",
        "                except Exception as e:\n",
        "                    print(f\"LLM error: {e}\")\n",
        "                    answer = f\"I encountered an error while generating the answer: {str(e)[:100]}\"\n",
        "            \n",
        "            # Clean up answer (remove repeated text)\n",
        "            if answer:\n",
        "                # Remove excessive repetition\n",
        "                words = answer.split()\n",
        "                if len(words) > 200:\n",
        "                    # If too long, take first 200 words\n",
        "                    answer = \" \".join(words[:200]) + \"...\"\n",
        "            \n",
        "            # Update chat history - prevent exact duplicates\n",
        "            if not chat_history or chat_history[-1] != (\"user\", question):\n",
        "                new_history = chat_history + [(\"user\", question), (\"assistant\", answer)]\n",
        "            else:\n",
        "                # Question already asked, just update answer\n",
        "                new_history = chat_history[:-1] + [(\"assistant\", answer)]\n",
        "            \n",
        "            return {\n",
        "                \"answer\": answer,\n",
        "                \"chat_history\": new_history\n",
        "            }\n",
        "'''\n",
        "\n",
        "# Apply fixes using regex\n",
        "# Fix query method - match method definition and body\n",
        "query_pattern = r'(\\s+def query\\(self, question: str\\):.*?)(?=\\s+def |\\s+class |\\Z)'\n",
        "agents_code = re.sub(query_pattern, new_query_method + '\\n', agents_code, flags=re.DOTALL)\n",
        "\n",
        "# Fix generate_answer method\n",
        "generate_pattern = r'(\\s+def generate_answer\\(state\\):.*?)(?=\\s+def |\\s+# Add nodes|\\s+workflow\\.add_node|\\s+workflow\\.set_entry_point)'\n",
        "agents_code = re.sub(generate_pattern, new_generate_answer + '\\n', agents_code, flags=re.DOTALL)\n",
        "\n",
        "# Write fixed file\n",
        "with open('agents.py', 'w') as f:\n",
        "    f.write(agents_code)\n",
        "\n",
        "print(\"‚úÖ Fixed agents.py successfully!\")\n",
        "print(\"   ‚úì Fixed query() method to extract answer correctly\")\n",
        "print(\"   ‚úì Fixed generate_answer() to prevent infinite loops\")\n",
        "print(\"   ‚úì Added duplicate detection and chat history limits\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Initialize the System\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all modules\n",
        "from pdf_processor import MultimodalPDFProcessor\n",
        "from vector_store import MultimodalVectorStore\n",
        "from agents import MultiAgentRAG\n",
        "import config\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Clean up existing vector store if it exists (to avoid ChromaDB conflicts)\n",
        "# Also fix for Colab readonly database issue\n",
        "vector_store_path = os.path.abspath(config.VECTOR_STORE_PATH)\n",
        "if os.path.exists(vector_store_path):\n",
        "    try:\n",
        "        # Close any existing ChromaDB connections first\n",
        "        import chromadb\n",
        "        try:\n",
        "            # Try to delete the database files with proper permissions\n",
        "            for root, dirs, files in os.walk(vector_store_path):\n",
        "                for file in files:\n",
        "                    try:\n",
        "                        file_path = os.path.join(root, file)\n",
        "                        os.chmod(file_path, 0o644)  # Make writable\n",
        "                    except:\n",
        "                        pass\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        shutil.rmtree(vector_store_path)\n",
        "        print(f\"‚úÖ Cleaned up existing vector store: {vector_store_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not clean up vector store: {e}\")\n",
        "        # Try to use a unique path instead to avoid readonly issues\n",
        "        import time\n",
        "        unique_path = f\"{vector_store_path}_{int(time.time())}\"\n",
        "        print(f\"üîÑ Using unique path instead: {unique_path}\")\n",
        "        os.environ[\"VECTOR_STORE_PATH\"] = unique_path\n",
        "        config.VECTOR_STORE_PATH = unique_path\n",
        "\n",
        "# Initialize vector store\n",
        "print(\"üì¶ Initializing vector store...\")\n",
        "try:\n",
        "    vector_store = MultimodalVectorStore()\n",
        "    print(f\"‚úÖ Vector store initialized with: {vector_store.embedding_type} embeddings\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error initializing vector store: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n",
        "\n",
        "# Initialize RAG system\n",
        "print(\"ü§ñ Initializing RAG system...\")\n",
        "try:\n",
        "    rag_system = MultiAgentRAG(vector_store, use_huggingface_primary=True)\n",
        "    print(f\"‚úÖ RAG system initialized with: {rag_system.primary_llm_type} LLM\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error initializing RAG system: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n",
        "\n",
        "print(\"\\nüéâ System ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Upload and Process PDF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import re\n",
        "import importlib\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# CRITICAL FIX: Ensure vector_store.py has the numpy array fix applied\n",
        "# This is a safety check in case Step 5 didn't catch it\n",
        "try:\n",
        "    # Find vector_store.py in current directory or nested directories\n",
        "    # NOTE: Use 'file_list' instead of 'files' to avoid shadowing google.colab.files\n",
        "    vs_path = None\n",
        "    for root, dirs, file_list in os.walk('.'):\n",
        "        if 'vector_store.py' in file_list:\n",
        "            vs_path = os.path.join(root, 'vector_store.py')\n",
        "            break\n",
        "    \n",
        "    if vs_path is None:\n",
        "        vs_path = 'vector_store.py'  # Fallback to current directory\n",
        "    \n",
        "    with open(vs_path, 'r') as f:\n",
        "        vs_content = f.read()\n",
        "    \n",
        "    # Check if the fix is already applied - look for the problematic pattern\n",
        "    needs_fix = False\n",
        "    if 'if image_embedding:' in vs_content:\n",
        "        # Check if it's NOT already fixed\n",
        "        lines = vs_content.split('\\n')\n",
        "        for i, line in enumerate(lines):\n",
        "            if 'if image_embedding:' in line and 'is not None' not in line:\n",
        "                needs_fix = True\n",
        "                print(f\"üîß Found problematic line {i+1}: {line.strip()}\")\n",
        "                break\n",
        "    \n",
        "    if needs_fix:\n",
        "        print(\"üîß Applying numpy array fix to vector_store.py...\")\n",
        "        # Fix all variations using regex - be very specific\n",
        "        vs_content = re.sub(\n",
        "            r'(\\s+)if\\s+image_embedding\\s*:',\n",
        "            r'\\1if image_embedding is not None:',\n",
        "            vs_content\n",
        "        )\n",
        "        with open(vs_path, 'w') as f:\n",
        "            f.write(vs_content)\n",
        "        print(\"‚úÖ Fixed numpy array check in vector_store.py\")\n",
        "        \n",
        "        # Reload the module\n",
        "        if 'vector_store' in sys.modules:\n",
        "            importlib.reload(sys.modules['vector_store'])\n",
        "        print(\"‚úÖ Reloaded vector_store module\")\n",
        "    else:\n",
        "        print(\"‚úÖ Numpy array fix already applied\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Could not apply fix: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "# Upload PDF file\n",
        "print(\"üì§ Upload your PDF file...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Process PDF\n",
        "processor = MultimodalPDFProcessor(processing_mode=config.PDF_PROCESSING_MODE)\n",
        "\n",
        "all_chunks = []\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith('.pdf'):\n",
        "        print(f\"\\nüìÑ Processing {filename}...\")\n",
        "        chunks = processor.process_pdf(filename)\n",
        "        all_chunks.extend(chunks)\n",
        "        print(f\"‚úÖ Extracted {len(chunks)} chunks from {filename}\")\n",
        "\n",
        "# Add to vector store\n",
        "if all_chunks:\n",
        "    print(f\"\\nüíæ Adding {len(all_chunks)} chunks to vector store...\")\n",
        "    try:\n",
        "        vector_store.add_documents(all_chunks)\n",
        "        print(\"‚úÖ Documents added successfully!\")\n",
        "    except ValueError as e:\n",
        "        if \"truth value of an array\" in str(e):\n",
        "            print(\"‚ùå Error: Numpy array truth value issue detected.\")\n",
        "            print(\"üîß Attempting to fix vector_store.py and retry...\")\n",
        "            # Apply fix and reload\n",
        "            with open('vector_store.py', 'r') as f:\n",
        "                vs_content = f.read()\n",
        "            vs_content = re.sub(\n",
        "                r'if\\s+image_embedding\\s*:',\n",
        "                'if image_embedding is not None:',\n",
        "                vs_content\n",
        "            )\n",
        "            with open('vector_store.py', 'w') as f:\n",
        "                f.write(vs_content)\n",
        "            # Reload and retry\n",
        "            if 'vector_store' in sys.modules:\n",
        "                importlib.reload(sys.modules['vector_store'])\n",
        "            from vector_store import MultimodalVectorStore\n",
        "            vector_store = MultimodalVectorStore()\n",
        "            vector_store.add_documents(all_chunks)\n",
        "            print(\"‚úÖ Fixed and documents added successfully!\")\n",
        "        else:\n",
        "            raise\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No chunks extracted from PDF\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Ask Questions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ask a question about your documents\n",
        "question = \"tell me about the image in doc\"  # Change this to your question\n",
        "\n",
        "print(f\"‚ùì Question: {question}\\n\")\n",
        "print(\"ü§î Thinking...\\n\")\n",
        "\n",
        "try:\n",
        "    answer = rag_system.query(question)\n",
        "    print(f\"\\nüí¨ Answer:\\n{answer}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Interactive Chat (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple interactive chat loop\n",
        "print(\"üí¨ Chat with your documents (type 'quit' to exit)\\n\")\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "while True:\n",
        "    question = input(\"\\nYou: \")\n",
        "    \n",
        "    if question.lower() in ['quit', 'exit', 'q']:\n",
        "        print(\"üëã Goodbye!\")\n",
        "        break\n",
        "    \n",
        "    if not question.strip():\n",
        "        continue\n",
        "    \n",
        "    try:\n",
        "        print(\"ü§î Thinking...\")\n",
        "        answer = rag_system.query(question)\n",
        "        print(f\"\\nü§ñ Assistant: {answer}\")\n",
        "        chat_history.append((question, answer))\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error: {e}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
