{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìö Multi-modal PDF RAG with LangGraph - Complete Google Colab Guide\n",
        "\n",
        "**Run this on GPU for best performance!**\n",
        "\n",
        "This notebook provides a complete multi-modal RAG system that can:\n",
        "- Process PDFs with text, images, and tables\n",
        "- Perform semantic search on both text and images  \n",
        "- Answer questions using a multi-agent system\n",
        "- Use Hugging Face models (free, no OpenAI required)\n",
        "\n",
        "## ‚öôÔ∏è Setup Steps:\n",
        "1. **Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí GPU (T4 or better)\n",
        "2. **Get Hugging Face API key**: https://huggingface.co/settings/tokens\n",
        "3. **Run all cells in order**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install All Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install all required packages\n",
        "!pip install -q langchain>=0.1.0 langchain-openai>=0.0.2 langchain-community>=0.0.10 langgraph>=0.0.20\n",
        "!pip install -q unstructured[pdf] pypdf pdf2image Pillow\n",
        "!pip install -q chromadb faiss-cpu\n",
        "!pip install -q sentence-transformers torch torchvision\n",
        "!pip install -q duckduckgo-search tavily-python\n",
        "!pip install -q python-dotenv requests opencv-python\n",
        "!pip install -q numpy==1.24.3 pydantic>=2.7.4,<3.0.0\n",
        "\n",
        "# Install system dependencies\n",
        "!apt-get update -qq\n",
        "!apt-get install -y -qq poppler-utils tesseract-ocr\n",
        "\n",
        "print(\"‚úÖ All dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Set Your API Keys\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# ‚ö†Ô∏è REPLACE WITH YOUR ACTUAL API KEYS ‚ö†Ô∏è\n",
        "HUGGINGFACE_API_KEY = \"YOUR_HUGGINGFACE_API_KEY_HERE\"  # Required - Get from https://huggingface.co/settings/tokens\n",
        "TAVILY_API_KEY = \"YOUR_TAVILY_API_KEY_HERE\"  # Optional - For web search\n",
        "\n",
        "# Set environment variables\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = HUGGINGFACE_API_KEY\n",
        "os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY\n",
        "os.environ[\"USE_HUGGINGFACE_PRIMARY\"] = \"true\"\n",
        "os.environ[\"USE_OPENAI_EMBEDDINGS\"] = \"false\"\n",
        "os.environ[\"USE_OPENAI_FALLBACK\"] = \"false\"\n",
        "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
        "\n",
        "print(\"‚úÖ API keys configured!\")\n",
        "print(f\"‚úì Hugging Face API key set: {bool(HUGGINGFACE_API_KEY and HUGGINGFACE_API_KEY != 'YOUR_HUGGINGFACE_API_KEY_HERE')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create Configuration File\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create config.py\n",
        "config_code = '''\"\"\"Configuration settings for the Multi-modal RAG system.\"\"\"\n",
        "import os\n",
        "\n",
        "# API Keys\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
        "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\", \"\")\n",
        "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\", \"\")\n",
        "\n",
        "# Model Configuration\n",
        "LLM_MODEL = \"gpt-4-1106-preview\"\n",
        "VISION_MODEL = \"gpt-4-vision-preview\"\n",
        "EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
        "\n",
        "# Hugging Face Configuration\n",
        "HUGGINGFACE_LLM_MODEL = os.getenv(\"HUGGINGFACE_LLM_MODEL\", \"microsoft/DialoGPT-medium\")\n",
        "HUGGINGFACE_MULTIMODAL_MODEL = \"Salesforce/blip-image-captioning-large\"\n",
        "USE_HUGGINGFACE_PRIMARY = os.getenv(\"USE_HUGGINGFACE_PRIMARY\", \"true\").lower() == \"true\"\n",
        "USE_OPENAI_EMBEDDINGS = os.getenv(\"USE_OPENAI_EMBEDDINGS\", \"false\").lower() == \"true\"\n",
        "USE_OPENAI_FALLBACK = os.getenv(\"USE_OPENAI_FALLBACK\", \"false\").lower() == \"true\"\n",
        "\n",
        "# Vector Store Configuration\n",
        "VECTOR_STORE_PATH = os.getenv(\"VECTOR_STORE_PATH\", \"./vector_store\")\n",
        "CHROMA_COLLECTION_NAME = \"multimodal_pdf_rag\"\n",
        "MAX_RETRIEVAL_DOCS = 3\n",
        "MAX_IMAGES_PER_QUERY = 2\n",
        "\n",
        "# PDF Processing Configuration\n",
        "PDF_PROCESSING_MODE = \"hi_res\"\n",
        "CHUNK_SIZE = 1000\n",
        "CHUNK_OVERLAP = 200\n",
        "\n",
        "# Agent Configuration\n",
        "MAX_ITERATIONS = 30\n",
        "TEMPERATURE = 0.0\n",
        "\n",
        "# Rate Limit Configuration\n",
        "MAX_RETRIES = 3\n",
        "RETRY_DELAY_SECONDS = 2\n",
        "'''\n",
        "\n",
        "with open('config.py', 'w') as f:\n",
        "    f.write(config_code)\n",
        "\n",
        "print(\"‚úÖ Created config.py\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Create Image Embeddings Module\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create image_embeddings.py\n",
        "image_embeddings_code = '''\"\"\"Image embedding module using CLIP for semantic image search.\"\"\"\n",
        "import base64\n",
        "from io import BytesIO\n",
        "from typing import List, Optional\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    CLIP_AVAILABLE = True\n",
        "except ImportError:\n",
        "    CLIP_AVAILABLE = False\n",
        "\n",
        "class ImageEmbedder:\n",
        "    \"\"\"Image embedding using CLIP model.\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name: str = \"clip-ViT-B-32\"):\n",
        "        self.model = None\n",
        "        self.model_name = model_name\n",
        "        if CLIP_AVAILABLE:\n",
        "            try:\n",
        "                import torch\n",
        "                device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "                self.model = SentenceTransformer(self.model_name, device=device)\n",
        "                print(f\"‚úì Loaded CLIP model: {self.model_name} on {device}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not load CLIP model: {e}\")\n",
        "    \n",
        "    def is_available(self) -> bool:\n",
        "        \"\"\"Check if CLIP is available.\"\"\"\n",
        "        return self.model is not None\n",
        "    \n",
        "    def embed_image(self, image_base64: str) -> Optional[np.ndarray]:\n",
        "        \"\"\"Generate embedding for a base64-encoded image.\"\"\"\n",
        "        if not self.is_available():\n",
        "            return None\n",
        "        \n",
        "        try:\n",
        "            # Decode base64 image\n",
        "            image_data = base64.b64decode(image_base64)\n",
        "            image = Image.open(BytesIO(image_data))\n",
        "            \n",
        "            # Generate embedding\n",
        "            embedding = self.model.encode(image, convert_to_numpy=True)\n",
        "            return embedding\n",
        "        except Exception as e:\n",
        "            print(f\"Error embedding image: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def embed_images(self, images_base64: List[str]) -> List[Optional[np.ndarray]]:\n",
        "        \"\"\"Generate embeddings for multiple images.\"\"\"\n",
        "        if not self.is_available():\n",
        "            return [None] * len(images_base64)\n",
        "        \n",
        "        embeddings = []\n",
        "        for img_b64 in images_base64:\n",
        "            emb = self.embed_image(img_b64)\n",
        "            embeddings.append(emb)\n",
        "        return embeddings\n",
        "'''\n",
        "\n",
        "with open('image_embeddings.py', 'w') as f:\n",
        "    f.write(image_embeddings_code)\n",
        "\n",
        "print(\"‚úÖ Created image_embeddings.py\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Clone Repository from GitHub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository from GitHub\n",
        "!git clone https://github.com/DevXSoni021/Multi_model_RAG_Langgraph.git\n",
        "%cd Multi_model_RAG_Langgraph\n",
        "\n",
        "print(\"‚úÖ Repository cloned successfully!\")\n",
        "print(\"üìÅ All Python files are now available in the current directory\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Initialize the System\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all modules\n",
        "from pdf_processor import MultimodalPDFProcessor\n",
        "from vector_store import MultimodalVectorStore\n",
        "from agents import MultiAgentRAG\n",
        "import config\n",
        "\n",
        "# Initialize vector store\n",
        "print(\"üì¶ Initializing vector store...\")\n",
        "vector_store = MultimodalVectorStore()\n",
        "print(f\"‚úÖ Vector store initialized with: {vector_store.embedding_type} embeddings\")\n",
        "\n",
        "# Initialize RAG system\n",
        "print(\"ü§ñ Initializing RAG system...\")\n",
        "rag_system = MultiAgentRAG(vector_store, use_huggingface_primary=True)\n",
        "print(f\"‚úÖ RAG system initialized with: {rag_system.primary_llm_type} LLM\")\n",
        "\n",
        "print(\"\\nüéâ System ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Upload and Process PDF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload PDF file\n",
        "print(\"üì§ Upload your PDF file...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Process PDF\n",
        "processor = MultimodalPDFProcessor(processing_mode=config.PDF_PROCESSING_MODE)\n",
        "\n",
        "all_chunks = []\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith('.pdf'):\n",
        "        print(f\"\\nüìÑ Processing {filename}...\")\n",
        "        chunks = processor.process_pdf(filename)\n",
        "        all_chunks.extend(chunks)\n",
        "        print(f\"‚úÖ Extracted {len(chunks)} chunks from {filename}\")\n",
        "\n",
        "# Add to vector store\n",
        "if all_chunks:\n",
        "    print(f\"\\nüíæ Adding {len(all_chunks)} chunks to vector store...\")\n",
        "    vector_store.add_documents(all_chunks)\n",
        "    print(\"‚úÖ Documents added successfully!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No chunks extracted from PDF\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Ask Questions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ask a question about your documents\n",
        "question = \"tell me about the image in doc\"  # Change this to your question\n",
        "\n",
        "print(f\"‚ùì Question: {question}\\n\")\n",
        "print(\"ü§î Thinking...\\n\")\n",
        "\n",
        "try:\n",
        "    answer = rag_system.query(question)\n",
        "    print(f\"\\nüí¨ Answer:\\n{answer}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Interactive Chat (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple interactive chat loop\n",
        "print(\"üí¨ Chat with your documents (type 'quit' to exit)\\n\")\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "while True:\n",
        "    question = input(\"\\nYou: \")\n",
        "    \n",
        "    if question.lower() in ['quit', 'exit', 'q']:\n",
        "        print(\"üëã Goodbye!\")\n",
        "        break\n",
        "    \n",
        "    if not question.strip():\n",
        "        continue\n",
        "    \n",
        "    try:\n",
        "        print(\"ü§î Thinking...\")\n",
        "        answer = rag_system.query(question)\n",
        "        print(f\"\\nü§ñ Assistant: {answer}\")\n",
        "        chat_history.append((question, answer))\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error: {e}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
