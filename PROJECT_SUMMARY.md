# Project Summary: Multi-modal PDF RAG with LangGraph

## âœ… Project Complete!

This project implements a complete **Multi-modal PDF RAG (Retrieval-Augmented Generation) system** using **LangGraph** for multi-agent orchestration.

## ğŸ“ Project Structure

```
Multi-modal-agent-pdf-RAG-with-langgraph/
â”œâ”€â”€ app.py                 # Streamlit web application
â”œâ”€â”€ main.py                # Application entry point
â”œâ”€â”€ config.py              # Configuration and settings
â”œâ”€â”€ pdf_processor.py       # PDF processing with multimodal support
â”œâ”€â”€ vector_store.py        # Vector database for embeddings
â”œâ”€â”€ agents.py              # LangGraph multi-agent system
â”œâ”€â”€ example_usage.py       # Example script for programmatic usage
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ .env.example          # Environment variables template
â”œâ”€â”€ .gitignore            # Git ignore rules
â”œâ”€â”€ README_SETUP.md       # Detailed setup instructions
â”œâ”€â”€ QUICKSTART.md         # Quick start guide
â”œâ”€â”€ readme.md             # Original project readme
â””â”€â”€ imgs/                 # Project images
    â”œâ”€â”€ BertAndGPT.jpg
    â”œâ”€â”€ LangGraph.png
    â”œâ”€â”€ Lang_Smith.png
    â”œâ”€â”€ LangSmith.png
    â”œâ”€â”€ multi_model_vector_retriever.png
    â””â”€â”€ RAG.png
```

## ğŸ¯ Key Features

### 1. **Multi-modal PDF Processing** (`pdf_processor.py`)
- Extracts text, images, and tables from PDFs
- Uses `unstructured` library for high-quality extraction
- Supports multiple processing modes (fast, OCR, hi-res)
- Encodes images as base64 for storage

### 2. **Vector Store** (`vector_store.py`)
- Uses ChromaDB for persistent vector storage
- OpenAI embeddings for semantic search
- Stores multimodal content (text + images)
- Retrieval with metadata filtering

### 3. **Multi-Agent System** (`agents.py`)
- **Supervisor Agent**: Orchestrates the workflow
- **Retriever Agent**: Searches PDF documents
- **Web Search Agent**: Searches the web for current information
- Uses LangGraph for stateful multi-agent coordination
- Intelligent routing based on query type

### 4. **Streamlit UI** (`app.py`)
- User-friendly web interface
- PDF upload and processing
- Interactive chat interface
- Document management
- Real-time query processing

## ğŸ”§ Technology Stack

- **LangChain**: Framework for LLM applications
- **LangGraph**: Multi-agent orchestration
- **OpenAI**: LLM and embeddings
- **ChromaDB**: Vector database
- **Unstructured**: PDF processing
- **Streamlit**: Web interface
- **Tavily/DuckDuckGo**: Web search

## ğŸš€ Getting Started

1. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

2. **Set up environment:**
   ```bash
   cp .env.example .env
   # Edit .env and add your OPENAI_API_KEY
   ```

3. **Run the application:**
   ```bash
   streamlit run app.py
   ```

## ğŸ“Š Architecture

```
User Query
    â†“
Supervisor Agent (LangGraph)
    â†“
    â”œâ”€â†’ Retriever Agent â†’ PDF Vector Store
    â”‚
    â””â”€â†’ Web Search Agent â†’ Internet
    â†“
Combined Response
```

## ğŸ“ How It Works

1. **PDF Processing**: PDFs are processed to extract text, images, and tables
2. **Embedding**: Content is embedded and stored in a vector database
3. **Query Routing**: Supervisor agent decides which agent to use:
   - **Retriever**: For questions about uploaded PDFs
   - **Web Search**: For current information not in PDFs
4. **Response Generation**: Selected agent processes the query and returns results
5. **Iteration**: Process continues until supervisor decides to finish

## ğŸ”‘ Configuration

Key settings in `config.py`:
- `LLM_MODEL`: GPT model for agents (default: "gpt-4-1106-preview")
- `VISION_MODEL`: For image processing (default: "gpt-4-vision-preview")
- `EMBEDDING_MODEL`: For embeddings (default: "text-embedding-3-large")
- `CHUNK_SIZE`: Text chunk size (default: 1000)
- `MAX_ITERATIONS`: Max agent iterations (default: 15)

## ğŸ“ Usage Examples

### Via Streamlit UI
1. Upload PDF files
2. Process them
3. Ask questions in the chat

### Programmatically
```python
from pdf_processor import MultimodalPDFProcessor
from vector_store import MultimodalVectorStore
from agents import MultiAgentRAG

# Process PDF
processor = MultimodalPDFProcessor()
chunks = processor.process_pdf("document.pdf")

# Create vector store
vector_store = MultimodalVectorStore()
vector_store.add_documents(chunks)

# Create RAG system
rag = MultiAgentRAG(vector_store)

# Query
answer = rag.query("What is this document about?")
```

## ğŸ› Troubleshooting

- **API Key Issues**: Ensure `.env` file has correct `OPENAI_API_KEY`
- **PDF Processing**: Check PDF is not corrupted and has sufficient disk space
- **Import Errors**: Run `pip install -r requirements.txt`
- **Web Search**: Install `duckduckgo-search` or set `TAVILY_API_KEY`

## ğŸ“š Documentation

- `README_SETUP.md`: Detailed setup instructions
- `QUICKSTART.md`: Quick start guide
- `example_usage.py`: Code examples
- `readme.md`: Original project documentation

## ğŸ‰ Next Steps

- Customize models and parameters in `config.py`
- Add more agents or tools
- Deploy to production (Streamlit Cloud, AWS, etc.)
- Add monitoring with LangSmith
- Enhance image processing capabilities

## ğŸ“„ License

This project is based on the original work by Wei Zhang. See `readme.md` for contact information.

---

**Built with â¤ï¸ using LangGraph, LangChain, and OpenAI**

